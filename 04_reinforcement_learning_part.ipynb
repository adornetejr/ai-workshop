{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.layers import fully_connected\n",
    "from statistics import median, mean\n",
    "from collections import Counter\n",
    "from time import sleep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Show 10 times how OpenAI Gym works. Plot images into screen and show how many steps were executed.\n",
    "# Randomly Actionable.\n",
    "#\n",
    "env = gym.make('CartPole-v0')\n",
    "env.reset()\n",
    "for _ in range(10):\n",
    "    observation = env.reset()\n",
    "    sleep(0.5)\n",
    "    for t in range(100):\n",
    "        plt.imshow(env.render(mode='rgb_array'))\n",
    "        action = env.action_space.sample()\n",
    "        observation, reward, done, info = env.step(action)\n",
    "        if done:\n",
    "            print(\"Episode finished after {} timesteps\".format(t+1))\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Observe the environment and run accordingly with. If pole in the left move to left, if the right move to the right.\n",
    "# Actionable according to the environment.\n",
    "#\n",
    "env = gym.make('CartPole-v0')\n",
    "env.reset()\n",
    "highscore = 0\n",
    "for _ in range(10):\n",
    "    observation = env.reset()\n",
    "    sleep(0.5)\n",
    "    points = 0\n",
    "    while True:\n",
    "        plt.imshow(env.render(mode='rgb_array'))\n",
    "        action = 1 if observation[2] > 0 else 0\n",
    "        observation, reward, done, info = env.step(action)\n",
    "        points += reward\n",
    "        if done:\n",
    "            if points > highscore:\n",
    "                highscore = points\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Run as many games as  `initial_games` and save the ones which pass the `score_requirement`. \n",
    "# Use all these knowledge to train your network after.\n",
    "#\n",
    "#\n",
    "goal_steps = 200\n",
    "score_requirement = 100\n",
    "initial_games = 50000\n",
    "\n",
    "train_data = []\n",
    "labels = []\n",
    "scores = []\n",
    "accepted_scores = []\n",
    "for _ in range(initial_games):\n",
    "    score = 0\n",
    "    game_memory = []\n",
    "    prev_observation = []\n",
    "    for _ in range(goal_steps):\n",
    "        action = random.randrange(0,2)\n",
    "        observation, reward, done, info = env.step(action)\n",
    "        if len(prev_observation) > 0 :\n",
    "            game_memory.append([prev_observation, action])\n",
    "        prev_observation = observation\n",
    "        score+=reward\n",
    "        if done: break\n",
    "\n",
    "    if score >= score_requirement:\n",
    "        accepted_scores.append(score)\n",
    "        for data in game_memory:\n",
    "            if data[1] == 1:\n",
    "                output = [0,1]\n",
    "            elif data[1] == 0:\n",
    "                output = [1,0]\n",
    "                    \n",
    "            train_data.append(data[0])\n",
    "            labels.append(output)\n",
    "\n",
    "    env.reset()\n",
    "    scores.append(score)\n",
    "    \n",
    "    \n",
    "print('Average accepted score:', mean(accepted_scores))\n",
    "print('Median score for accepted scores:', median(accepted_scores))\n",
    "print(Counter(accepted_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Starting training the Neural Network...')\n",
    "print('Train data size: ', len(train_data))\n",
    "train_data = np.array([i for i in train_data]).reshape(-1,len(train_data[0]))\n",
    "labels = np.array(labels)\n",
    "\n",
    "now = datetime.utcnow().strftime(\"%d/%m at %H:%M:%S\")\n",
    "root_log = \"tf_logs\"\n",
    "logdir = \"{}/run-{}/\".format(root_log, now)\n",
    "\n",
    "n_inputs = 4\n",
    "n_hidden = 144\n",
    "n_outputs = 2\n",
    "dropout_keep_prob = 0.6\n",
    "\n",
    "learning_rate = 0.1\n",
    "\n",
    "n_epochs = 200\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs))\n",
    "y = tf.placeholder(tf.int32, shape=(None))\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden_1 = fully_connected(X, n_hidden)\n",
    "    dropout_1 = tf.nn.dropout(hidden_1, keep_prob=dropout_keep_prob)\n",
    "    \n",
    "    hidden_2 = fully_connected(dropout_1, n_hidden)\n",
    "    dropout_2 = tf.nn.dropout(hidden_2, keep_prob=dropout_keep_prob)\n",
    "\n",
    "    \n",
    "    hidden_3 = fully_connected(dropout_2, n_hidden)\n",
    "    dropout_3 = tf.nn.dropout(hidden_3, keep_prob=dropout_keep_prob)\n",
    "    \n",
    "    logits = fully_connected(dropout_3, n_outputs, activation_fn=tf.nn.softmax)\n",
    "\n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.keras.backend.categorical_crossentropy(target=tf.cast(y, tf.float32), output=logits)\n",
    "    loss = tf.reduce_mean(xentropy)\n",
    "    loss_summary = tf.summary.scalar('Loss', loss)\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    training = optimizer.minimize(loss)\n",
    "\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.keras.metrics.top_k_categorical_accuracy(y, tf.cast(logits, tf.float32), 2)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "file_writer = tf.summary.FileWriter(logdir, tf.get_default_graph())\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "\n",
    "    for epoch in range(n_epochs + 1):\n",
    "        X_batch = train_data\n",
    "        y_batch = labels\n",
    "        sess.run(training, feed_dict={X: X_batch, y: y_batch})\n",
    "        acc_train = accuracy.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "        acc_loss = loss.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "        summary = loss_summary.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "        file_writer.add_summary(summary, epoch)\n",
    "        print('Epoch: {} of {}, Accuracy: {}, Loss: {} \\r'.format(epoch, n_epochs, acc_train, acc_loss), end=\"\")\n",
    "            \n",
    "\n",
    "    save_path = saver.save(sess, './final.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Load the trained neural network and run 5 games. We get all moves which finish the game (200 moves) and\n",
    "# add to training data. \n",
    "# After running the 5 games, re-run again the training part to improve more the neural network. \n",
    "#\n",
    "score_requirement = 200\n",
    "accepted_scores = []\n",
    "\n",
    "dropout_keep_prob = 1.0\n",
    "\n",
    "env = gym.make('CartPole-v0')\n",
    "env.reset()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, \"./final.ckpt\") \n",
    "\n",
    "    for each_game in range(5):\n",
    "        score = 0\n",
    "        game_memory = []\n",
    "        previous_observation = []\n",
    "        env.reset()\n",
    "        sleep(0.5)\n",
    "        for _ in range(goal_steps):\n",
    "            plt.imshow(env.render(mode='rgb_array'))\n",
    "\n",
    "            if len(observation)==0:\n",
    "                action = random.randrange(0,2)\n",
    "            else:\n",
    "                X_batch = observation.reshape(-1,len(observation))\n",
    "                z = logits.eval(feed_dict={X: X_batch})\n",
    "                action = np.argmax(z, axis=1)[0]\n",
    "        \n",
    "            observation, reward, done, info = env.step(action)\n",
    "                \n",
    "            if len(previous_observation) > 0 :\n",
    "                game_memory.append([previous_observation, action])\n",
    "            \n",
    "            previous_observation = observation\n",
    "            score+=reward\n",
    "            if done: break\n",
    "\n",
    "        if score >= score_requirement:\n",
    "            accepted_scores.append(score)\n",
    "            for data in game_memory:\n",
    "                if data[1] == 1:\n",
    "                    output = [0,1]\n",
    "                elif data[1] == 0:\n",
    "                    output = [1,0]\n",
    "                    \n",
    "                train_data = np.append(train_data, [data[0]], axis=0)\n",
    "                labels = np.append(labels, [output], axis=0)\n",
    "\n",
    "    print(Counter(accepted_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
